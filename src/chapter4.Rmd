---
title: 'Chapter 4'
subtitle: 'STAT 3240'
author: "Michael McIsaac"
institute: "UPEI"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      navigation:
        scroll: false # disable slide transitions by scrolling
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
duo_accent(primary_color = "#006747", secondary_color = "#CFC493",   
	header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Roboto Mono"))
```		




```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE, fig.align="center", fig.height=5.5)
options(DT.options = list(scrollX = TRUE, pageLength=20, scrollY = 300))


library(here)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(tidyverse)
library(ggplot2)
library(knitr)
library(mosaic)
library(DT)
library(car)
library(ALSM)

#params
spending_subset_all = read.csv(here("data", "spending_subset.csv"))

spending_subset=spending_subset_all[1:30,]

par(lwd=3,cex=1.5) 
cdi = as_tibble(read.delim(here("data", "CDI.txt"), sep=" ", header=FALSE)[,-c(1:2)] %>% mutate(V18 = recode_factor(V18, "NE", "NC", "S", "W")))
names(cdi) = c("county", "state", "land_area", "population", "pop_18_to_34", "pop_65", "number_physicians", "number_hospital_beds", "total_serious_crimes", "high_school_grads", "bachelor_degrees", "poverty_rate", "unemployment_rate", "per_capita_income", "total_personal_income", "region")

tab_model <- function(...,  show.ci=.95){sjPlot::tab_model(...,  show.ci=show.ci, show.se=TRUE, collapse.ci=TRUE, show.stat=TRUE)}
```




### 4: Simultaneous Inferences and Other Topics in Regression Analysis

### Learning Objectives for Sections 4.1-4.3 

After Sections 4.1-4.3, you should be able to 

- Compute and interpret Bonferroni and Working-Hotelling simultaneous CIs
- Compute and interpret simultaneous prediction intervals
	
	
---

### 4.1 Joint Estimation of $\beta_0$ and $\beta_1$

A procedure that provides a family confidence coefficient when estimating both $\beta_0$ and $\beta_1$ is often highly desirable since it permits the analyst to weave the two separate results together into an integrated set of conclusions, with an assurance that the entire set of estimates is correct. 

---

### Bonferroni Joint Confidence Intervals

One procedure for constructing simultaneous confidence intervals for $\beta_0$ and $\beta_1$ with a specified family confidence coefficient is the Bonferroni procedure:

$$\boxed{b_0 \pm t(1-(\alpha/2)/2; n-2)s \{ b_0 \}  }.$$
$$\boxed{b_1 \pm t(1-(\alpha/2)/2; n-2)s \{ b_1 \}  }.$$

I.e., we use the usual CIs, but at a level of $1-\alpha/2$, so that together we have at least a simultaneous confidence of $1-\alpha$.

--

We reiterate that the Bonferroni $1-\alpha$ family confidence coefficient is actually a lower bound on the true (but often unknown) family confidence coefficient. 

To the extent that incorrect interval estimates of $\beta_0$ and $\beta_1$ tend to pair up in the family, the families of statements will tend to be correct more than $(1-\alpha) 100$ percent of the time. 

---

### 4.2 Simultaneous Estimation of Mean Responses


Often the mean responses at a number of $X$ levels need to be estimated from the same sample data.

The combination of sampling errors in $b_0$ and $b_1$ may be such that the interval estimates of $E[Y_h]$ will be correct over some range of $X$ levels and incorrect elsewhere.


---

### Working-Hotelling Procedure

The Working-Hotelling procedure is based on the confidence band for the regression line discussed in Section 2.6. 


The *confidence band* contains the entire regression line and therefore contains the mean responses at all $X$ levels. Hence, we can use the boundary values of the confidence band at selected $X$ levels as simultaneous estimates of the mean responses at these $X$ levels. 

The family confidence coefficient for these simultaneous estimates will be at least $1-\alpha$ because the confidence coefficient that the entire confidence band for the regression line is correct is $1-\alpha$.

$$\boxed{\hat{Y}_h \pm \sqrt{2 F(1-\alpha; 2; n-2)} s \{ \hat{Y}_h \}  }$$

---

### Bonferroni Procedure

The Bonferroni procedure, discussed earlier for simultaneous estimation of $\beta_0$ and $\beta_1$ is a completely general procedure. 

To construct a family of confidence intervals for mean responses at $g$ different $X$ levels with this procedure with family confidence coefficient $1-\alpha$, we use  

$$\boxed{\hat{Y}_h \pm t(1-(\alpha/2)/g; n-2) s \{ \hat{Y}_h \}  }$$

---

For larger families, the Working-Hotelling confidence limits will always be the tighter, since $\sqrt{2 F(1-\alpha; 2; n-2)}$ stays the same for any number of statements in the family,  whereas $t(1-(\alpha/2)/g; n-2)$ becomes larger as the number of statements increases. 

In practice, once the family confidence coefficient has been decided upon, one can calculate both of these terms to determine which procedure leads to tighter confidence limits.

Note that both the Working-Hotelling and Bonferroni procedures provide lower bounds to the actual family confidence coefficient.

---
layout: true
class: inverse

---

### SHS: Simultaneous Estimation


The Canadian Survey of Household Spending is carried out annually across Canada. 

(http://dli-idd-nesstar.statcan.gc.ca.proxy.library.upei.ca/webview/) 

The main purpose of the survey is to obtain detailed information about household spending. Information is also collected about dwelling characteristics as well as household equipment. 

The survey data are used by the following groups:

* Government departments use the data to help formulate policy; 
* Community groups, social agencies and consumer groups use the data to support their positions and to lobby governments for social changes; 
* Lawyers and their clients use the data to determine what is fair for child support and other compensation; 
* Labour and contract negotiators rely on the data when discussing wage and cost-of-living clauses; 
* Individuals and families can use the data to compare their spending habits with those of similar types of households. 

---

```{r, include=TRUE}
###A subset of the latest Survey of Household Spending data are displayed below:
spending_subset %>% datatable()
```

We are interested in the potential relationship between the income of working Canadians and the amount that they spend on clothing in a year.


---

Income and Clothing Expenditure for a small subset of the Survey of Household Spending are displayed below:
 
```{r, fig.cap="", include=TRUE, message=FALSE, echo=TRUE, eval=TRUE}
xyplot(clothing_expenditure~income, data=spending_subset,  type=c("p", "r", "smooth"))
```

---
 
```{r, fig.cap="", include=TRUE, message=FALSE, echo=TRUE, eval=TRUE}
clothing_model = lm(clothing_expenditure~income, data=spending_subset)
confint(clothing_model, level=.95) %>% round(4)
confint(clothing_model, level=.975) %>% round(4)
```

--

* **Interpret the confidence intervals in your own words.**

--

Notice that the $[1-(\alpha/2)] \cdot 100$ percentile of the $t$-distribution is used in order to get a confidence level of $1-\alpha$. 


Similarly the $[1-(\alpha/2)/2] \cdot 100$ percentile of the $t$-distribution is used in order to get a joint confidence level of $1-(\alpha/2)$. 

---

A family of *confidence intervals* for simultaneous estimation of $E[Y_h]$ corresponding to $X_h =$ 40000, 50000, and 60000 can be found using the following code in `R`:
```{r,include=TRUE}
bonf_ci=predict(clothing_model, newdata=data.frame(income=c(40000, 50000, 60000)), interval="confidence", level=1-0.1/3); bonf_ci
WH_ci=ALSM::ci.reg(clothing_model, newdata=data.frame(income=c(40000, 50000, 60000)), alpha=0.1, type="w"); WH_ci
```

---

```{r}
B = qt(1-0.1/(2*3), clothing_model$df.residual); B
W = sqrt(2* qf(1-.1, 2, clothing_model$df.residual)); W
```

---

* **Interpret the confidence intervals in your own words.**

<!-- PUT SELECT WARM UP RESPONSES HEREHERE (preceded by a ">"): -->
<!-- warm up question 4) 1 -->



---
layout: false

### 4.3: Simultaneous Prediction Intervals for New Observations

Two procedures for making simultaneous predictions will be considered here: the Scheffe and Bonferroni procedures. Both utilize the same type of limits as those for predicting a single observation, and only the multiple of the estimated standard deviation is changed. 

The Scheffe procedure uses the $F$ distribution, whereas the Bonferroni procedure uses the $t$ distribution. 

The simultaneous prediction limits for $g$ predictions with the Scheffe procedure with family confidence coefficient $1 - \alpha$ are:

$$\hat Y_h \pm S \cdot s\{pred\},$$
where $S^2= g \cdot F(1 - \alpha; g, n-2)$. 

The Bonferroni procedure uses
$$\hat Y_h \pm B \cdot s\{pred\},$$
where $B= t(1 - (\alpha/2)/g; n-2)$.

---

The $S$ and $B$ multiples can be evaluated in advance to see which procedure provides tighter prediction limits.

Note that both the $B$ and $S$ multiples for simultaneous predictions become larger as $g$ increases. 

This contrasts with simultaneous estimation of mean responses where the $B$ multiple becomes larger but not the $W$ (Working-Hotelling) multiple. 

When $g$ is large, both the $B$ and $S$ multiples for simultaneous predictions may become so large that the prediction intervals will be too wide to be useful. 

Other simultaneous estimation techniques might then be considered, as discussed in Reference 4.1.


---
layout: true
class: inverse

---

### SHS: Simultaneous Prediction

Income and Clothing Expenditure for a small subset of the Survey of Household Spending are displayed below:
 
```{r, fig.cap="", include=TRUE, message=FALSE, echo=TRUE, eval=TRUE}
xyplot(clothing_expenditure~income, data=spending_subset,  type=c("p", "r", "smooth"))
```

---

A family of *prediction intervals* for simultaneous prediction of $Y_h$:

```{r, include=TRUE}
bonf_ci = predict(clothing_model, newdata=data.frame(income=c(40000, 50000, 60000)), interval="predict", level=1-0.1/3); bonf_ci
bonf_ci2 = ALSM::ci.reg(clothing_model, newdata=data.frame(income=c(40000, 50000, 60000)), alpha=0.1, type="gn"); bonf_ci2
  scheffe_ci = ALSM::ci.reg(clothing_model, newdata=data.frame(income=c(40000, 50000, 60000)), alpha=0.1, type="s"); scheffe_ci
```

--

Those `type="s"` intervals do not seem plausible...

---
Let's look at what `ci.reg` is doing when `type="s"`:

```{r, eval=TRUE}
ALSM::ci.reg
```

---

```{r, out.height=6} 
ci.reg_fixed = function (model, newdata, type = c("b", "s", "w", "n", "m", "nm", "gn"), alpha = 0.05, m = 1) 
{
    type <- match.arg(type)
    newdata <- as.data.frame(newdata)
    if (dim(newdata)[2] == length(names(model$coeff))) {
        colnames(newdata) <- names(model$coeff)
    }
    else {
        colnames(newdata) <- names(model$coeff)[-1]
    }
    CI <- predict(model, newdata, se.fit = T)
    g <- nrow(newdata)
    p <- ncol(newdata) + 1
    syh <- CI$se.fit
    spred <- sqrt(CI$residual.scale^2 + (CI$se.fit)^2)
    spredmean <- sqrt((CI$residual.scale^2)/m + (CI$se.fit)^2)
    b <- qt(1 - alpha/(2 * g), model$df)
    s <- sqrt(g * qf(1 - alpha, g, model$df))
    w <- sqrt(p * qf(1 - alpha, p, model$df))
    if (match.arg(type) == "b") {
        s <- syh
        z <- b
    }
    else if (match.arg(type) == "s") {
        {{z <- s}}
        {{s <- spred}}
    }
    else if (match.arg(type) == "w") {
        s <- syh
        z <- w
    }
    else if (match.arg(type) == "n") {
        s <- spred
        z <- qt(1 - alpha/2, model$df)
    }
    else if (match.arg(type) == "nm") {
        s <- spredmean
        z <- qt(1 - alpha/2, model$df)
    }
    else if (match.arg(type) == "m") {
        s <- syh
        z <- qt(1 - alpha/2, model$df)
    }
    else if (match.arg(type) == "gn") {
        s <- spred
        z <- b
    }
    x <- data.frame(newdata, Fit = CI$fit, Lower.Band = CI$fit - z * s, Upper.Band = CI$fit + z * s)
    return(x)
}
```

---
.pull-left[
```{r, include=TRUE}
bonf_ci = predict(clothing_model, newdata=data.frame(income=c(40000, 50000, 60000)), interval="predict", level=1-0.1/3); bonf_ci
scheffe_ci = ALSM::ci.reg(clothing_model, newdata=data.frame(income=c(40000, 50000, 60000)), alpha=0.1, type="s"); scheffe_ci[,c(3,4)]
S = sqrt(3*qf(1-.1, 3, clothing_model$df.residual)); S
(6717.366- -2150.273) / (6069.400- -1502.3075)
```
]

.pull-right[
```{r, include=TRUE}
bonf_ci2 = ALSM::ci.reg(clothing_model, newdata=data.frame(income=c(40000, 50000, 60000)), alpha=0.1, type="gn"); bonf_ci2[,c(3,4)]
scheffe_ci = ci.reg_fixed(clothing_model, newdata=data.frame(income=c(40000, 50000, 60000)), alpha=0.1, type="s"); scheffe_ci[,c(3,4)]
B = qt(1-.1/(2*3), clothing_model$df.residual); B
S/B
```
]

---

* **Interpret the prediction intervals in your own words.**

<!-- PUT SELECT WARM UP RESPONSES HEREHERE (preceded by a ">"): -->
<!-- warm up question 4) 2 -->



---

### CDI: Simultaneous CIs and PIs - physicians vs hospital beds

This data set provides selected county demographic information (CDI) for 440 of the most populous counties in the United States. 

Each line of the data set has an identification number with a county name and state abbreviation and provides information on 14 variables for a single county. 

Counties with missing data were deleted from the data set.

---
```{r , echo = TRUE}
cdi %>% datatable()
```

---

```{r,  fig.cap="", include=TRUE}
xyplot(number_physicians ~ number_hospital_beds, data=cdi,  type=c("p", "r", "smooth"))
```

---

```{r,  echo = TRUE}
mod_physician_beds = lm(number_physicians ~ number_hospital_beds, data=cdi)
predict(mod_physician_beds, newdata=data.frame(number_hospital_beds=c(1500, 5000, 15000, 25000)), interval="confidence", level=1-0.1/4)
predict(mod_physician_beds, newdata=data.frame(number_hospital_beds=c(1500, 5000, 15000, 25000)), interval="predict", level=1-0.1/4)
```


* **Interpret these intervals in your own words.**


---
### CDI: physicians vs population 


```{r,  fig.cap="", include=TRUE, fig.height=4}
xyplot(number_physicians ~ population, data=cdi,  type=c("p", "r", "smooth"))
```

---

```{r,  echo = TRUE}
mod_physician_pop = lm(number_physicians ~ population, data=cdi)
predict(mod_physician_pop, newdata=data.frame(population=c(200000, 400000, 4000000, 6000000, 8000000)), interval="confidence", level=1-0.05/5)
ci.reg(mod_physician_pop, newdata=data.frame(population=c(200000, 400000, 4000000, 6000000, 8000000)), alpha=0.05, type="w")
```

---

```{r}
B = qt(1-0.05/(2*5), mod_physician_pop$df.residual); B
W = sqrt(2* qf(1-.05, 2, mod_physician_pop$df.residual)); W
```

---

```{r,  echo = TRUE}
mod_physician_pop = lm(number_physicians ~ population, data=cdi)
predict(mod_physician_pop, newdata=data.frame(population=c(200000, 400000, 4000000, 6000000, 8000000)), interval="predict", level=1-0.05/5)
ci.reg_fixed(mod_physician_pop, newdata=data.frame(population=c(200000, 400000, 4000000, 6000000, 8000000)), alpha=0.05, type="s")
```

---

```{r}
B = qt(1-.05/(2*5), mod_physician_pop$df.residual); B
S = sqrt(5*qf(1-.05, 5, mod_physician_pop$df.residual)); S
```


---
layout: false


### Recap: Sections 4.1-4.3 

After Sections 4.1-4.3, you should be able to 

- Compute and interpret Bonferroni and Working-Hotelling simultaneous CIs
- Compute and interpret simultaneous prediction intervals
	
	


---
### Learning Objectives for Sections 4.5, 4.7 

After Sections 4.5 and 4.7, you should be able to 

- Understand the potential impact of measurement error
- Understand the challenges of choosing X levels when designing an experiment
	

---
###  4.5: Effects of Measurement Errors

---

### Measurement Errors in $Y$

When random measurement errors are present in the observations on the response variable $Y$, no new problems are created when these errors are uncorrelated and not biased (positive and negative measurement errors tend to cancel out). 

--

Consider, for example, a study of the relation between the time required to complete a task $(Y)$ and the complexity of the task $(X)$. The time to complete the task may not be measured accurately because the person operating the stopwatch may not do so at the precise instants called for. 

As long as such measurement errors are of a random nature, uncorrelated, and not biased, these measurement errors are simply absorbed in the model error term $\varepsilon$. 

--

The model error term always reflects the composite effects of a large number of factors not considered in the model, one of which now would be the random variation due to inaccuracy in the process of measuring $Y$.


---

layout: true
class: inverse

---
### SHS: Measurement Error in $Y$

Income and Clothing Expenditure for a small subset of the Survey of Household Spending are displayed below:

```{r, echo=FALSE}
spending_subset=spending_subset_all[1:500,]
```

```{r}
random_error = rnorm(n=length(spending_subset$income), mean=0, sd=sd(spending_subset$clothing_expenditure)/2)
spending_subset$clothing_expenditure_plus_error = spending_subset$clothing_expenditure + random_error

spending_subset[, c("income", "clothing_expenditure", "clothing_expenditure_plus_error")] %>% round(3) %>% datatable()
```

---

```{r, fig.cap="", include=TRUE, message=FALSE, echo=TRUE, eval=TRUE}
xyplot(clothing_expenditure~income, data=spending_subset,  type=c("p", "r"), ylim=c(-2000, 10000))
```

---

```{r, fig.cap="", include=TRUE, message=FALSE, echo=TRUE, eval=TRUE}
xyplot(clothing_expenditure_plus_error~income, data=spending_subset,  type=c("p", "r"),  ylim=c(-2000, 10000))
```


---
```{r, echo=TRUE, message=FALSE}
msummary(lm(clothing_expenditure~income, data=spending_subset)) 
msummary(lm(clothing_expenditure_plus_error~income, data=spending_subset))
```

---
```{r, echo=TRUE, message=FALSE}
anova(lm(clothing_expenditure~income, data=spending_subset)) 
anova(lm(clothing_expenditure_plus_error~income, data=spending_subset))
```


---
layout: false

### Measurement Errors in X

Unfortunately, a different situation holds when the observations on the predictor variable $X$ are subject to measurement errors. 

At times measurement errors may enter the value observed for the predictor variable, for instance, when the predictor variable is pressure in a tank, temperature in an oven, speed of a production line, or reported age of a person.

---

layout: true
class: inverse

---
### SHS: Measurement Error in $X$

Income and Clothing Expenditure for a small subset of the Survey of Household Spending are displayed below:

```{r}
random_error = rnorm(n=length(spending_subset$income), mean=0, sd=sd(spending_subset$income)/2)
spending_subset$income_plus_error = spending_subset$income + random_error

spending_subset[, c("income", "income_plus_error", "clothing_expenditure")] %>% datatable()
```

---

```{r, fig.cap="", include=TRUE, message=FALSE, echo=TRUE, eval=TRUE}
xyplot(clothing_expenditure~income, data=spending_subset,  type=c("p", "r"), xlim=c(0, 100000))
```

---
```{r, fig.cap="", include=TRUE, message=FALSE, echo=TRUE, eval=TRUE}
xyplot(clothing_expenditure~income_plus_error, data=spending_subset,  type=c("p", "r"), xlim=c(0, 100000))
```


---
```{r, echo=TRUE, message=FALSE}
msummary(lm(clothing_expenditure~income, data=spending_subset)) 
msummary(lm(clothing_expenditure~income_plus_error, data=spending_subset))
```


---
```{r, echo=TRUE, message=FALSE}
anova(lm(clothing_expenditure~income, data=spending_subset)) 
anova(lm(clothing_expenditure~income_plus_error, data=spending_subset))
```



---

.pull-left[
Introducing measurement error into $Y$
```{r , eval=c(1:3)}
X = 1:100
Y = X
plot(X, Y, ylim=c(-10, 110), xlim=c(-10, 110))

for (i in 1:1000) { 
Y = Y + rnorm(100)
plot(X, Y, ylim=c(-10, 110), xlim=c(-10, 110))
abline(lm(Y~X))
lines(c(-10, 110), c(-10, 110), col="grey", lty=2)}
```
]

.pull-right[
Introducing measurement error into $X$
```{r, eval=c(1:3)}
X = 1:100
Y = X
plot(X, Y, ylim=c(-10, 110), xlim=c(-10, 110))

for (i in 1:1000) { 
X = X + rnorm(100)
plot(X, Y, ylim=c(-10, 110), xlim=c(-10, 110))
abline(lm(Y~X))
lines(c(-10, 110), c(-10, 110), col="grey", lty=2)}
```
]

---

```{r, echo=FALSE}
set.seed(354)
```

.pull-left[
Introducing measurement error into $Y$
```{r measurement_error_Y, eval=TRUE, fig.show='animate', ffmpeg.format='gif', dev='jpeg', cache=TRUE, interval=0.025, aniopts="controls"}
X = 1:100
Y = X
plot(X, Y, ylim=c(-10, 110), xlim=c(-10, 110))

for (i in 1:5000) { 
Y = Y + rnorm(100)
plot(X, Y, ylim=c(-10, 110), xlim=c(-10, 110))
abline(lm(Y~X), lwd=2)
lines(c(-10, 110), c(-10, 110), col="grey", lty=2, lwd=2)}
```
]

.pull-right[
Introducing measurement error into $X$
```{r measurement_error_X, eval=TRUE, fig.show='animate', ffmpeg.format='gif', dev='jpeg', cache=TRUE, interval=0.025, aniopts="controls"}
X = 1:100
Y = X
plot(X, Y, ylim=c(-10, 110), xlim=c(-10, 110))

for (i in 1:5000) { 
X = X + rnorm(100)
plot(X, Y, ylim=c(-10, 110), xlim=c(-10, 110))
abline(lm(Y~X), lwd=2)
lines(c(-10, 110), c(-10, 110), col="grey", lty=2, lwd=2)}
```
]

---
layout: false

### Classical Measurement Error

The regression model that we would like to study is
$$ Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$

However, we observe only $X_i^\ast = X + \delta_i$, so we actually study $Y_i = \beta_0^\ast + \beta_1^\ast X_i^\ast + \varepsilon_i^\ast$  
--

Even if $E[X_i^\ast] = X_i$, we will not get the same estimates from these models. 

--

Notice that 
$$\begin{align*} Y_i &= \beta_0 + \beta_1 X_i + \varepsilon_i \\ &= \beta_0 + \beta_1 (X_i^\ast - \delta) + \varepsilon_i \\ &= \beta_0 + \beta_1 X_i^\ast + (\varepsilon_i  - \beta_1 \delta) \end{align*}$$
isn't an ordinary regression model because $X_i^\ast$ and $\delta$ are correlated. 

This leads to an *attenuation* of the parameter estimate.
In fact, we can show that 
$$\beta_1^\ast = \beta_1 \frac{\sigma_X^2}{\sigma_X^2 + \sigma_\delta^2}$$

---
layout: true
class: inverse
---



```{r, echo=TRUE, message=FALSE}
msummary(lm(clothing_expenditure~income, data=spending_subset)) 
msummary(lm(clothing_expenditure~income_plus_error, data=spending_subset))
```

$$\begin{align*} \beta_1^\ast &= \beta_1 \frac{\sigma_X^2}{\sigma_X^2 + \sigma_\delta^2} \\ &= \beta_1 \frac{\sigma_X^2}{\sigma_X^2 + \sigma_X^2/4} \\ &= \beta_1 (.8) \end{align*}$$

---
```{r, echo=FALSE}
spending_subset=spending_subset_all[1:30,]
```

* **If it was discovered that income was measured with error, how would that impact your interpretation of the regression output given above? **

<!-- PUT SELECT WARM UP RESPONSES HEREHERE (preceded by a ">"): -->
<!-- warm up question 4) 3 -->


---

layout: false

### 4.7: Choice of $X$ Levels

Consider the impact of the choice of $X$ levels:

```{r, fig.cap="", include=TRUE, message=FALSE, echo=FALSE, out.width="60%"}
include_graphics("img/Figure171.png")
```

---


Although the number and spacing of $X$ levels depends very much on the major purpose of the regression analysis, the general advice given by D. R. Cox is still relevant:

> Use two levels when the object is primarily to examine whether or not ... (the predictor variable) . . . has an effect and in which direction that effect is. 

> Use three levels whenever a description of the response curve by its slope and curvature is likely to be adequate; this should cover most cases. 

> Use four levels if further examination of the shape of the response curve is important. 

> Use more than four levels when it is required to estimate the detailed shape of the response curve, or when the curve is expected to rise to an asymptotic value, or in general to show features not adequately described by slope and curvature. Except in these last cases it is generally satisfactory to use equally spaced levels with equal numbers of observations per level.

---

layout: true
class: inverse

---
### SHS: Choice of $X$ Levels

Income and Clothing Expenditure for a small subset of the Survey of Household Spending are displayed below:

```{r, fig.height=3}
xyplot(clothing_expenditure~income, data=spending_subset,  type=c("p", "r"))
```

---

* ** If you could design an experiment where you controlled the values of income, how could that change your regression findings?  **

<!-- PUT SELECT WARM UP RESPONSES HEREHERE (preceded by a ">"): -->
<!-- warm up question 4) 4 -->





---
layout: false

### Recap: Sections 4.5, 4.7 

After Sections 4.5 and 4.7, you should be able to 

- Understand the potential impact of measurement error
- Understand the challenges of choosing $X$ levels when designing an experiment


---

### Examples of Measurement Error: 

*  Lester F, Arbuckle TE, Peng P, and McIsaac MA (2018). Impact of exposure to phenols during early pregnancy on birth weight in two Canadian Cohort studies subject to measurement errors. *Environment International*, 120, 231-237.

* Addressing measurement error in a cumulative exposure variable: the relationship between light
at night and breast cancer risk
