---
title: 'Chapter 5'
subtitle: 'STAT 3240'
author: "Michael McIsaac"
institute: "UPEI"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: xaringan-themer.css # xaringan-themer-print.css
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      navigation:
        scroll: false # disable slide transitions by scrolling
---


```{r xaringan-themer, include=FALSE}
library(xaringanthemer)
duo_accent(primary_color = "#006747", secondary_color = "#CFC493",   
	header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Droid Mono"))
```



```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE, fig.align="center", fig.height=5.5)
options(DT.options = list(scrollX = TRUE, pageLength=200, scrollY = 200))


library(here)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(tidyverse)
library(ggplot2)
library(knitr)
library(mosaic)
library(DT)
library(car)

#params
spending_subset_all = read.csv(here("data", "spending_subset.csv"))

spending_subset=spending_subset_all[1:30,]

par(lwd=3,cex=1.5) 
cdi = as_tibble(read.delim(here("data", "CDI.txt"), sep=" ", header=FALSE)[,-c(1:2)] %>% mutate(V18 = recode_factor(V18, "NE", "NC", "S", "W")))
names(cdi) = c("county", "state", "land_area", "population", "pop_18_to_34", "pop_65", "number_physicians", "number_hospital_beds", "total_serious_crimes", "high_school_grads", "bachelor_degrees", "poverty_rate", "unemployment_rate", "per_capita_income", "total_personal_income", "region")

tab_model <- function(...,  show.ci=.95){sjPlot::tab_model(...,  show.ci=show.ci, show.se=TRUE, collapse.ci=TRUE, show.stat=TRUE)}
```




### 5: Matrix Approach to Simple Linear Regression Analysis

---

### Learning Objectives for Sections 5.8-5.10 

After Sections 5.8-5.10, you should be able to 

- Write simple linear regression in matrix terms
- Write simple least squares estimation in matrix terms
	
	
---

### 5.9: Simple Linear Regression Model in Matrix Terms 

$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, \qquad i=1, \ldots, n$$

This means

$$\begin{align*}
Y_1 =& \beta_0 + \beta_1 X_1 + \varepsilon_1\\
Y_2 =& \beta_0 + \beta_1 X_2 + \varepsilon_2\\
& \vdots\\
Y_n =& \beta_0 + \beta_1 X_n + \varepsilon_n
\end{align*}$$

--

In matrix terms, 
$$\underset{n\times 1}{\mathbb{Y}} = \underset{n\times 2}{\mathbb{X}} \  \underset{2 \times 1}{\mathbb{\beta}} + \underset{n\times 1}{\mathbb{\varepsilon}}$$
--

$$\left[\begin{array}{c} Y_1\\  Y_2\\ \vdots\\ Y_n \end{array}\right] = \left[ \begin{array}{cc} 1 & X_1\\ 1 & X_2\\ \vdots&\vdots\\ 1 & X_n \end{array}\right] \left[\begin{array}{c} \beta_0\\ \beta_1 \end{array}\right] + \left[\begin{array}{c} \varepsilon_1\\ \varepsilon_2\\ \vdots\\ \varepsilon_n \end{array}\right]$$

---

Our usual regression assumptions:


$$E\{\underset{n\times 1}{\mathbb{\varepsilon}} \} =  E\left\{ \left[  \begin{array}{c} \varepsilon_1\\ \varepsilon_2\\ \vdots\\ \varepsilon_n \end{array} \right] \right\} = \left[ \begin{array}{c} E\left \{ \varepsilon_1 \right\}\\ E\left \{ \varepsilon_2 \right\}\\ \vdots\\ E\left \{\varepsilon_n \right\} \end{array} \right] =\left[ \begin{array}{c} 0 \\ 0 \\ \vdots\\ 0 \\ \end{array} \right] =  \underset{n\times 1}{\mathbb{0}}$$

$$\underset{n\times n}{\mathbb{\sigma^2 \{ \varepsilon\}}} = \left[  \begin{array}{ccc} \sigma^2 & 0  & \cdots & 0 \\ 0 & \sigma^2 & \cdots & 0 \\ \vdots & \vdots  & \ddots & \vdots  \\ 0 & 0 & \cdots & \sigma^2  \end{array} \right] = \sigma^2 \underset{n\times n}{\mathbb{ I }}$$

---

Note that the **Variance-Covariance Matrix of Random Vector** is 

$$\underset{n\times n}{\mathbb{\sigma^2 \{ Y \}}} = \left[  \begin{array}{ccc} \sigma^2\{Y_1\} & \sigma\{Y_1, Y_2\}  & \cdots & \sigma\{Y_1, Y_n\} \\ \sigma\{Y_2, Y_1\} & \sigma^2\{Y_2\} & \cdots & \sigma\{Y_2, Y_n\} \\ \vdots & \vdots  & \ddots & \vdots  \\ \sigma\{Y_n, Y_1\} & \sigma\{Y_n, Y_2\} & \cdots & \sigma^2\{Y_n\}  \end{array} \right] = E \{ [ \mathbb{Y} - E[\mathbb{Y}]] [ \mathbb{Y} - E[\mathbb{Y}]]' \}$$

```{r, fig.cap="", include=TRUE, message=FALSE, echo=FALSE, out.width="60%"}
include_graphics("../img/Figure195.png")
```

---

$$\begin{align*}  E\{\underset{n\times 1}{\mathbb{Y}} \} & = E\{ \underset{n\times 2}{\mathbb{X}} \  \underset{2 \times 1}{\mathbb{\beta}} + \underset{n\times 1}{\mathbb{\varepsilon}} \} \\ &  =  E\left \{ \left[ \begin{array}{cc} 1 & X_1\\ 1 & X_2\\ \vdots&\vdots\\ 1 & X_n \end{array}\right] \left[\begin{array}{c} \beta_0\\ \beta_1 \end{array}\right] + \left[\begin{array}{c} \varepsilon_1\\ \varepsilon_2\\ \vdots\\ \varepsilon_n \end{array}\right]\right\} \\ &  = E\left\{ \left[  \begin{array}{c} \beta_0 + \beta_1 X_1 + \varepsilon_1\\ \beta_0 + \beta_1 X_2 + \varepsilon_2\\ \vdots\\ \beta_0 + \beta_1 X_n + \varepsilon_n \end{array} \right] \right\} \\ &  = \left[ \begin{array}{c} E\left \{\beta_0 + \beta_1 X_1 + \varepsilon_1 \right\}\\ E\left \{\beta_0 + \beta_1 X_2 + \varepsilon_2 \right\}\\ \vdots\\ E\left \{\beta_0 + \beta_1 X_n + \varepsilon_n \right\} \end{array} \right]  = \left[ \begin{array}{c} \beta_0 + \beta_1 X_1 \\ \beta_0 + \beta_1 X_2 \\ \vdots\\ \beta_0 + \beta_1 X_n  \end{array} \right] \\ &  =  \underset{n\times 2}{\mathbb{X}} \  \underset{2 \times 1}{\mathbb{\beta}}  \end{align*}$$



---

### 5.10: Least Squares Estimation of Regression Parameters

To derive the normal equations by the method of least squares, we minimize the quantity
$$\begin{align*} Q &= \sum \left[ Y_i - (\beta_0 + \beta_1 X_i) \right]^2 \\ &= (\mathbb{Y} - \mathbb{X} \mathbb{\beta})' (\mathbb{Y} - \mathbb{X} \mathbb{\beta}) \\ &= \mathbb{Y}'\mathbb{Y} - \mathbb{\beta}' \mathbb{X}'  \mathbb{Y} - \mathbb{Y}' \mathbb{X} \mathbb{\beta} +  \mathbb{\beta}' \mathbb{X}' \mathbb{X} \mathbb{\beta}  \\ &= \mathbb{Y}'\mathbb{Y} - 2 \mathbb{\beta}' \mathbb{X}'  \mathbb{Y} +  \mathbb{\beta}' \mathbb{X}' \mathbb{X} \mathbb{\beta}  \end{align*}$$
So, we find the values $b$ such that 
$$\begin{align*} \mathbb{0} & = \left. \frac{ \mathbb{\partial}}{\mathbb{\partial \beta}}	Q \right |_{\beta=b} = \left. - 2 \mathbb{X}'  \mathbb{Y} + 2 \mathbb{X}' \mathbb{X} \mathbb{\beta} \right |_{\beta=b} \end{align*}$$

--

That is, 
$$\mathbb{b} = (\mathbb{X}' \mathbb{X})^{-1} \mathbb{X}'  \mathbb{Y} $$


---

Note that

$$\mathbb{X'X} =  \left[ \begin{array}{cccc} 1 & 1 & \cdots & 1 \\ X_1 & X_2 & \cdots & X_n \end{array}\right] \left[ \begin{array}{cc} 1 & X_1\\ 1 & X_2\\ \vdots&\vdots\\ 1 & X_n \end{array}\right] = \left[ \begin{array}{cc} n & \sum X_i  \\ \sum X_i  & \sum X_i^2 \end{array}\right]$$

and

$$\mathbb{X'Y} =  \left[ \begin{array}{cccc} 1 & 1 & \cdots & 1 \\ X_1 & X_2 & \cdots & X_n \end{array}\right] \left[\begin{array}{c} Y_1\\  Y_2\\ \vdots\\ Y_n \end{array}\right] = \left[ \begin{array}{c} \sum Y_i \\ \sum X_i Y_i \end{array}\right]$$

---

layout: true
class: inverse

---

*The Canadian Survey of Household Spending (http://dli-idd-nesstar.statcan.gc.ca.proxy.library.upei.ca/webview/) is carried out annually across Canada. The main purpose of the survey is to obtain detailed information about household spending. Information is also collected about dwelling characteristics as well as household equipment.*


```{r}
#A subset of the latest Survey of Household Spending data are displayed below:
spending_subset %>% datatable()
```

---

```{r}
clothing_model = lm(clothing_expenditure~income, data=spending_subset)
msummary(clothing_model)
```

---


For the Canadian Survey of Household Spending example, the matrix representation of the simple linear regression involves the following vectors and matrices:
```{r, fig.cap="", include=TRUE}
X = cbind(1, spending_subset$income)
Y = cbind(spending_subset$clothing_expenditure)
```

.pull-left[
```{r}
X %>%  datatable()
```
]

.pull-right[
```{r}
Y%>%  datatable()
```
]

---

.pull-left[
```{r}
# X'X
t(X) %*% X

# X'Y
t(X)%*% Y
```
]
.pull-right[
```{r}
# (X'X)^(-1)
solve(t(X) %*% X)

# (X'X)^(-1) X'Y
solve(t(X) %*% X) %*% t(X)%*% Y
```
]


--

* **Interpret $(X'X)^{-1} X'Y$ in your own words.**

<!-- PUT SELECT WARM UP RESPONSES HEREHERE (preceded by a ">"): -->
<!-- warm up question 5) 1 -->

*  This is a way to find the slope and intercept of a regression model.

* This is using to find the estimated regression coefficients by the matrix method.

---

* ** In your own words, describe the value of this matrix approach to simple linear regression analysis?**

<!-- PUT SELECT WARM UP RESPONSES HEREHERE (preceded by a ">"): -->
<!-- warm up question 5) 2 -->

*  the matrix approach using matrix to contain the values of error, predictor variables, response variable and other value, and do the operations base on the matrix algebra rules. It should be an efficient way for computer to arrange, store and do the calculations.

* It permits extensive systems of equations and large arrays of data to be denoted compactly and operated upon efficiently.

* **A matrix approach is not needed for simple linear regression, it is easier to use the method we have used earlier. But since we already know simple linear regression it is a good way of introducing matrix approach which will be very useful when we are dealing with multiple regression.**


---
layout: false

### Recap: Sections 5.8-5.10 

After Sections 5.8-5.10, you should be able to 

- Write simple linear regression in matrix terms
- Write simple least squares estimation in matrix terms

---

### Learning Objectives for Sections 5.11-5.13

After Sections 5.11-5.13, you should be able to

- Write fitted values and residuals in matrix terms	
-	Write ANOVA and regression inferences in matrix terms

---

### 5.11 Fitted Values and Residuals



In matrix terms,

$$\underset{n\times 1}{\mathbb{\hat Y}} = \underset{n\times 2}{\mathbb{X}} \  \underset{2 \times 1}{\mathbb{b}} $$

$$\left[\begin{array}{c} \hat Y_1\\ \hat  Y_2\\ \vdots\\ \hat Y_n \end{array}\right] = \left[ \begin{array}{cc} 1 & X_1\\ 1 & X_2\\ \vdots&\vdots\\ 1 & X_n \end{array}\right] \left[\begin{array}{c} b_0\\ b_1 \end{array}\right] =\left[ \begin{array}{c} b_0 + b_1 X_1 \\ b_0 + b_1 X_2 \\ \vdots\\ b_0 + b_1 X_n  \end{array} \right]$$
--

Remember that 
$$\mathbb{b} = (\mathbb{X}' \mathbb{X})^{-1} \mathbb{X}'  \mathbb{Y}$$

So, 
$${\mathbb{\hat Y}} = {\mathbb{X}} \  (\mathbb{X}' \mathbb{X})^{-1} \mathbb{X}'  \mathbb{Y}$$
--

We call $\underset{n\times n}{\mathbb{H}} = {\mathbb{X}} \  (\mathbb{X}' \mathbb{X})^{-1} \mathbb{X}'$ the **Hat Matrix** because it puts a hat on Y  
(i.e., $\mathbb{\hat Y} = \mathbb{H} \mathbb{Y})$ 

---

Note that the fitted values $\mathbb{\hat Y}$ are linear combinations of the observed values $\mathbb{Y}$ with weights given by the predictor variables through $\mathbb{H}={\mathbb{X}} \  (\mathbb{X}' \mathbb{X})^{-1} \mathbb{X}'.$

Looking at the hat matrix can tell us how much influence each observation of $X$ has on the overall fit; this is useful in diagnosing influential observations (Chapter 10). 

--

An important property of $\mathbb{H}$ is that it is symmetric $(\mathbb{H}' = \mathbb{H})$ and idempotent $(\mathbb{H} \mathbb{H} = \mathbb{H})$. 

Both of these properties should be obvious if you remember that 
$$\mathbb{H}={\mathbb{X}} \  (\mathbb{X}' \mathbb{X})^{-1} \mathbb{X}'.$$

---

### Residuals



In matrix notation, 
$$\begin{align*}  \underset{n\times 1}{\mathbb{e}} & = \underset{n \times 1}{\mathbb{Y}} - \underset{n\times 1}{\mathbb{\hat Y}} \\ & = \underset{n\times 1}{\mathbb{Y}} - \underset{n\times 2}{\mathbb{X}} \ \underset{2 \times 1}{\mathbb{b}} \\ & = \underset{n\times 1}{\mathbb{Y}} - \underset{n\times n}{\mathbb{H}} \ \underset{n\times 1}{\mathbb{Y}} \\ &= \left ( \underset{n\times n}{\mathbb{I}}- \underset{n\times n}{\mathbb{H}} \right ) \underset{n\times 1}{\mathbb{Y}} \end{align*}$$

--

Remember that $\sigma^2\{aX\} = a^2 \sigma^2\{X\}.$ 

Similarly, 
$$\sigma^2\{\mathbb{A Y}\} = \mathbb{A}\ \sigma^2\{\mathbb{Y}\} \ \mathbb{A'}.$$


Remember also that $\sigma^2\{\mathbb{Y}\} = \sigma^2 \mathbb{I}.$

---

So, the variance-covariance matrix of the residuals can be expressed as 
$$\begin{align*}  \underset{n\times n}{\mathbb{\sigma^2\{e\}}} & = \left ( {\mathbb{I}}- {\mathbb{H}} \right ) \mathbb{\sigma^2\{Y\}} \left ( {\mathbb{I}}- {\mathbb{H}} \right )' \\ & = \left ( {\mathbb{I}}- {\mathbb{H}} \right ) \left ( \sigma^2 \mathbb{I} \right ) \left ( {\mathbb{I}}- {\mathbb{H}} \right )' \\ & = \sigma^2 \left ( {\mathbb{I}}- {\mathbb{H}} \right ) \left ( {\mathbb{I}}- {\mathbb{H}} \right ) \\ 
&= \sigma^2 \left ( {\mathbb{I}}- {\mathbb{H}} \right )  \end{align*}$$
is estimated by 
$$\begin{align*}  \underset{n\times n}{\mathbb{s^2\{e\}}} & = MSE \left ( \underset{n\times n}{\mathbb{I}}- \underset{n\times n}{\mathbb{H}} \right )  \end{align*}$$

--

Note that, like $\underset{n\times n}{\mathbb{H}}$, the matrix $\underset{n\times n}{\mathbb{I}}- \underset{n\times n}{\mathbb{H}}$ is symmetric and idempotent. 


---

### 5.12 Analysis of Variance Results

$$\begin{align*}SSE = \mathbb{e}'\mathbb{e} & = (\mathbb{Y} - \mathbb{X} \mathbb{b})' (\mathbb{Y} - \mathbb{X} \mathbb{b}) \\ &= \mathbb{Y}'\mathbb{Y}  - 2 \mathbb{b}' \mathbb{X}' \mathbb{Y} + \mathbb{b}' \mathbb{X}' \mathbb{X} \mathbb{b} \\ &= \mathbb{Y}'\mathbb{Y}  - 2 \mathbb{b}' \mathbb{X}' \mathbb{Y} + \mathbb{b}' \mathbb{X}' \mathbb{X} (\mathbb{X}'\mathbb{X})^{-1} \mathbb{X}' \mathbb{Y} \\ &= \mathbb{Y}'\mathbb{Y}  - 2 \mathbb{b}' \mathbb{X}' \mathbb{Y} + \mathbb{b}' \mathbb{X}' \mathbb{Y} \\ &= \mathbb{Y}'\mathbb{Y}  -  \mathbb{b}' \mathbb{X}' \mathbb{Y} \\ \end{align*}$$

--
$$\begin{align*} SSTO = \sum (Y_i - \bar Y) ^2 = \mathbb{Y}'\mathbb{Y} - \frac{1}{n} \mathbb{Y}' \mathbb{J}  \mathbb{Y} \end{align*},$$

where 
$$\begin{align*} \mathbb{J} = \left [\begin{array}{cccc} 1 & 1 & \cdots & 1 \\ 1 & 1 & \cdots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & \cdots & 1  \end{array} \right] \end{align*}$$

--

$$\begin{align*} SSR &= SSTO - SSE = \mathbb{b}' \mathbb{X}' \mathbb{Y} - \frac{1}{n} \mathbb{Y}' \mathbb{J}  \mathbb{Y} \end{align*}.$$

---

Note that we can write these in *quadratic form* (i.e., in the form $\mathbb{Y}'\mathbb{A}\mathbb{Y}$, where $\mathbb{A}$ is symmetric):

$$\begin{align*} SSTO & = \mathbb{Y}'\mathbb{Y} - \frac{1}{n} \mathbb{Y}' \mathbb{J}  \mathbb{Y} = \mathbb{Y}'(\mathbb{I} - \frac{1}{n} \mathbb{J}  ) \mathbb{Y} \\
SSE &= \mathbb{Y}'\mathbb{Y}  -  \mathbb{b}' \mathbb{X}' \mathbb{Y} = \mathbb{Y}'\mathbb{Y}  -  \mathbb{\hat Y}' \mathbb{Y} = \mathbb{Y}'\mathbb{Y}  -  \mathbb{Y}' \mathbb{H}' \mathbb{Y} = \mathbb{Y}'(\mathbb{I}  - \mathbb{H}) \mathbb{Y}\\
SSR &= \mathbb{b}' \mathbb{X}' \mathbb{Y} - \frac{1}{n} \mathbb{Y}' \mathbb{J}  \mathbb{Y} = \mathbb{\hat Y}' \mathbb{Y} - \frac{1}{n} \mathbb{Y}' \mathbb{J}  \mathbb{Y}= \mathbb{Y}'\mathbb{H} \mathbb{Y} - \frac{1}{n} \mathbb{Y}' \mathbb{J}  \mathbb{Y} = \mathbb{Y}' (\mathbb{H} - \frac{1}{n} \mathbb{J})  \mathbb{Y}
\end{align*}$$



---

### 5.13 Inferences in Regression Analysis

The variance-covariance matrix of $\mathbb{b}$, the estimator of $\mathbb{\beta}$,
$$\underset{2 \times 2}{\sigma^2\{\mathbb{b}\}} =   \left[ \begin{array}{cc} \sigma^2\{\mathbb{b_0}\} & \sigma\{\mathbb{b_0, b_1}\}  \\ \sigma\{\mathbb{b_1, b_0}\}  & \sigma^2\{\mathbb{b_1}\} \end{array}\right]$$
is the variance
$$\begin{align*} \sigma^2\{\mathbb{b}\} &= \sigma^2\{\mathbb{(X'X)^{-1}X'Y}\}\} \\ &= \mathbb{(X'X)^{-1}X'} \sigma^2 \{  \mathbb{Y} \} \mathbb{X (X'X)^{-1}} \\ &=  \mathbb{(X'X)^{-1}X'} (\sigma^2 \mathbb{I}) \mathbb{X (X'X)^{-1}} \\ &= \sigma^2 \mathbb{(X'X)^{-1} (X'X) (X'X)^{-1}} \\ &=  \sigma^2 (\mathbb{X}'\mathbb{X})^{-1} \end{align*}$$

---
So,

$$\underset{2 \times 2}{\sigma^2 \{\mathbb{b}\} } =   \sigma^2 (\mathbb{X}'\mathbb{X})^{-1} =  \sigma^2 \left[ \begin{array}{cc} \frac{1}{n} + \frac{ \bar {X} ^2 }{\sum (X_i - \bar X)^2} & \frac{ -\bar {X}  }{\sum (X_i - \bar X)^2}  \\ \frac{ - \bar {X} }{\sum (X_i - \bar X)^2}  & \frac{ 1 }{\sum (X_i - \bar X)^2} \end{array}\right]$$
--

The estimated variance-covariance matrix of $\mathbb{b}$ is 
$$\underset{2 \times 2}{\sigma^2\{\mathbb{b}\}} =   MSE (\mathbb{X}'\mathbb{X})^{-1}$$

---

### Mean Response

To estimate the mean response at $X_h$, we define the vector 
$$\underset{	2 \times 1}{\mathbb{X_h}} =   \left[ \begin{array}{c} 1 \\ X_h \end{array}\right],$$
so we can write 
$$\hat {Y_h} = \mathbb{X_h^\prime b} =  \left[ \begin{array}{cc} 1 & X_h \end{array}\right] \left[ \begin{array}{c} b_0 \\ b_1 \end{array}\right] = [b_0 + b_1 X_h].$$

--

This has variance
$$\sigma^2\{\hat {Y_h}\} = \sigma^2\{\mathbb{X_h^\prime b}\} = \mathbb{X_h^\prime} \sigma^2\{\mathbb{b}\} \mathbb{X_h} = \mathbb{X_h^\prime} \sigma^2 \mathbb{(X'X)^{-1}} \mathbb{X_h} =  \sigma^2 \mathbb{X_h^\prime} \mathbb{(X'X)^{-1}} \mathbb{X_h}$$

--
Note that this reduces to the familiar expression
$$\sigma^2\{\hat {Y_h}\} = \sigma^2 \left[ \frac{1}{n} + \frac{ (X_h - \bar X)^2 }{\sum (X_i - \bar X)^2}  \right],$$
where we can see explicitly that the variance expression contains contributions from $\sigma^2\{b_0\}, \sigma^2\{b_1\}$, and $\sigma\{b_0, b_1\}$ which it must since $\hat Y_h = b_0 + b_1 X_h$ is a linear combination of $b_0$ and $b_1$.

---

### Prediction of New Observation

$$s^2\{pred\} = MSE(1 + \mathbb{X_h^\prime} \mathbb{(X'X)^{-1}} \mathbb{X_h})$$

---

layout: true
class: inverse

---

*The Canadian Survey of Household Spending (http://dli-idd-nesstar.statcan.gc.ca.proxy.library.upei.ca/webview/) is carried out annually across Canada. The main purpose of the survey is to obtain detailed information about household spending. Information is also collected about dwelling characteristics as well as household equipment.*


```{r}
#A subset of the latest Survey of Household Spending data are displayed below:
spending_subset %>% datatable()
```

---

```{r}
clothing_model = lm(clothing_expenditure~income, data=spending_subset)
msummary(clothing_model)
```

---


For the Canadian Survey of Household Spending example, the matrix representation of the simple linear regression involves the following vectors and matrices:
```{r, fig.cap="", include=TRUE}
X = cbind(1, spending_subset$income)
Y = cbind(spending_subset$clothing_expenditure)
```

.pull-left[
```{r}
X %>%  datatable()
```
]

.pull-right[
```{r}
Y%>%  datatable()
```
]

---

.pull-left[
```{r}
# X'X
t(X) %*% X

# X'Y
t(X)%*% Y
```
]

.pull-right[
```{r}
# (X'X)^(-1)
solve(t(X) %*% X)

# (X'X)^(-1) X'Y
solve(t(X) %*% X) %*% t(X)%*% Y
```
]

* **What are the dimensions of the hat matrix here?** 
	- $(30 \times 30)$

* **Find $s^2\{pred\}$ for someone with an income equal to `$`60,000:** 
	- `(msummary(clothing_model)$sigma)^2 * (1+cbind(1, 60000)%*%solve(t(X) %*% X)%*% t(cbind(1, 60000)))` $= 2929101$

---
.pull-left[
```{r}
# Y
Y%>%  datatable()
```
]

.pull-right[
```{r}
#  X (X'X)^(-1) X'Y
X %*% solve(t(X) %*% X) %*% t(X)%*% Y                 %>% datatable()
```
]


* **What are the residuals corresponding to the first 3 individuals?** $(1062.552, -597.5183, 361.4183)$



---
layout: false

### Recap: Sections 5.11-5.13

After Sections 5.11-5.13, you should be able to

- Write fitted values and residuals in matrix terms	
-	Write ANOVA and regression inferences in matrix terms




